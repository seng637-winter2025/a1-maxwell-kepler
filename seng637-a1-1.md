>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 2 |
|-----------------|
| Student 1 Maxwell Kepler |   
| Student 2 Riley Martel |      


**Table of Contents**

[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was divided](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction
In the lab, we explored different testing methodologies on an ATM system, focusing on functionality testing without access to the underlying implementation. The system under test (SUT) was designed to serve multiple user types, customers, and operators. Working in our pair helped simulate real development team dynamics and lead to collaborative learning experiences across our team. We documented the findings in an Excel Spreadsheet, which let us track which functions did and didn’t have bugs, then tracked any issues in JIRA. 

Our testing began with exploratory testing, where we worked together as a pair to discover and evaluate the system's behavior. Before this lab, I understood exploratory testing as a flexible, intuitive approach where testers investigate a system's functionality based on their understanding of requirements, without explicitly defined test plans. This method allows testers to adapt their strategy as they learn more about the system's behavior and potential weaknesses. At this stage, we started by reading the requirements for the ATM simulation as outlined in the appendix to get a better understanding of what the ATM's behaviour should look like. Then, we devised a test plan from what we had learned, focusing on testing most functions a little bit, and taking both the common and exceptional paths to do such. Since we are a group of 2, we took turns driving the simulation, where the other pattern would write down what features were tested, and which bugs were found. 

The second phase involved manual scripted testing, where we executed pre-written test cases provided to us. Prior to this lab, I knew that manual scripted testing offered a more structured approach compared to exploratory testing. A benefit of manual scripted testing is that it ensured a consistent coverage of critical functionalities and made it easier to track test results and system behavior. Similar to our approach in the exploratory testing, one of us drove the code while the other tracked what tests had been performed. 

Finally, we conducted regression testing on version 1.1 of the ATM system to verify whether previously identified issues had been resolved and to ensure no new bugs were introduced. For this stage, to divide up the work evenly, we checked each aspect that we had each reported, and performed this task separately. Since we didn’t add the version number into the Jira board, we solely tracked the regression testing in our Excel Spreadsheet.

## Test Types
We performed the three following types of test:
- Exploratory Testing
- Manual Scripted (Functional) Testing
- Regression Testing
##

# High-level description of the exploratory testing plan
As part of our paired testing approach, we developed our strategy by first first familiarizing ourselves with the ATM simulation requirements in Appendix B. Since exploratory testing emphasizes discovering system behavior without extensive pre-planning, we chose to focus deeply on a subset of key functionalities, exploring both common user paths and less common scenarios. This approach allowed us to thoroughly understand and test critical system components. 

Our exploratory testing process followed these key steps:
## Initial System Analysis
- Reviewed the ATM's functionality comprehensively through the provided documentation.
- Identified critical functions that required thorough testing.
- Mapped out potential user interactions and system responses.

## Testing Approach
- Decided to focus on depth rather than breadth for key functionalities.
- Prioritized testing of security-critical features.
- Planned to explore both common user paths and edge cases.

## Test Documentation and Bug Tracking
- We documented each test case with clear initial states in an Excel Spreadsheet.
- Recorded step-by-step procedures.
- Noted both expected and actual outcomes.
- Maintained a pass/fail record for each test scenario.
- Once we had completed all of the testing, we added each bug to our JIRA project. 

## Test Documentation Strategy
We were able to perform the exploratory testing efficiently by:
- Documented each test case with clear initial states.
- Recorded step-by-step procedures.
- Noted both expected and actual outcomes.
- Maintained a pass/fail record for each test scenario.

## Test Execution Strategy
- Implemented pair testing with defined driver/observer roles.
- Rotated responsibilities to ensure thorough coverage.

## Scope of Testing
We structured our exploration around the different key functional areas that represented core ATM operations. 

1. Security and Authentication - We prioritized testing the card and PIN validation mechanisms as they formed the security foundation of the system. This included:
- Card validation scenarios (valid, invalid, and edge cases).
- PIN entry process with multiple attempt scenarios.
- Security lockout procedures.
2. Financial Transactions - For transaction testing, we concentrated on:
- Various withdrawal and deposit amounts and scenarios.
- Transfer operations between different account types.
- Edge cases like maximum limits and invalid amounts.
3. Account Information Access - We explored the account inquiry system by:
- Testing balance checks across different account types.
- Verifying account information accuracy.
- Testing scenarios with non-existent accounts.
4. System Administration - For operational functionality, we focused on:
- System startup procedures.
- Cash balance verification.
- Operator controls and shutdown processes.

## Test Logistics
The following is a list of each team member, and their reported test cases.

| Member | Test Case # |
| --- | --- |
| Maxwell Kepler | 1-15 |
| Riley Martel | 16-30 |

# Comparison of exploratory and manual functional testing
The Exploratory testing focused on testing one function with multiple different inputs rather than testing the input flow for each function in the manual scripted testing. This allowed for more bugs to be found, when it came to those specific core functions. However, due to the nature of the exploratory testing, functions were tested haphazardly (out of order), and certain things were not tested at all compared to the scripted testing which focused on user flow. The manual scripted tests were more efficient as multiple tests were related, i.e does selecting deposit, show accounts to select? When selecting the account, does it prompt the user to enter a dollar amount? After inputting the amount does the system request the envelope? Because of the sequential nature of the tests, the efficiency was much higher. This sequential nature also allowed for the tests to not focus on the end result/final function, as compared to the exploratory testing. Both approaches complement each other however, allowing for a more comprehensive testing solution as opposed to using just one approach.

# Notes and discussion of the peer reviews of defect reports
Due to the size of our group (2) we were unable to do a peer review. However we did discuss some interesting quirks we found in the ATM simulation. For example we were trying to ascertain what the definition of available balance meant. We had 2 thoughts, it was reflecting the amount of money within the ATM (however after more testing this wasn’t the case). Our second thought was that the ATM puts a hold on all deposits for an unknown amount of time, thus preventing the user from taking out quickly deposited money.

We also had an instance where we both found the same bug, however only one of us tested it with multiple inputs, rather than just a singular input.


# How the pair testing was managed and team work/effort was divided 
Our pair testing approach was structured around maximizing efficiency. For the different testing phases, we implemented the following organization.

## Exploratory Testing Phase
After reading through the assignment handout, we chose to use the driver-observer pattern where one team member operated the ATM simulation while the other documented the test cases and observations. After one group member had been driving for about half an hour, we would switch roles to ensure that both team members got hands-on experience with the system. As well, both team members contributed equally to ensure fairness and thorough coverage.

## Manual Scripted Testing Phase
For this phase, we continued on using the driver-observer pattern. As well, since the test cases were provided to us for this portion, we divided the workload in half. We maintained detailed records of test outcomes, with the observer documenting any deviations from expected results

## Regression Testing Phase
We divided the regression testing based on our initial findings, with each team member responsible for verifying the fixes for bugs they originally documented. This division allowed us to work efficiently in parallel while maintaining familiarity with the previously identified issue. As well, we cross-checked each other's findings to ensure thorough verification.


# Difficulties encountered, challenges overcome, and lessons learned
Our main difficulty was the overall structure of the assignment as certain sections were  unclear. It was also mildly infuriating that the ATM simulation did not have a reset function or something to manually edit account/card information. The ability to do so would have greatly increased the precision of our reported bugs. We learned how to effectively think of how to test software for bugs, especially how to track what has been found and tested already. We also underestimated how long it would take to implement the JIRA part of our assignment, and would have been better to do so earlier on.

# Comments/feedback on the lab and lab document itself
The lab was well designed to balance simplicity with meaningful learning outcomes. While the testing tasks could be repetitive, the structured progression through different testing methodologies kept us engaged and provided valuable hands-on experience with different testing scenarios.

## Strengths of the Lab
- Clear progression through different testing methodologies.
- Appropriate level of complexity that allowed for effective team collaboration.

## Areas of Improvement
- The introduction to JIRA could have been more comprehensive.
- The guideline on the best practices felt poorly organized, and a better example could have been provided.
- The lab was not designed for groups of 2 instead of groups of 4.
- The structure of the lab document felt unorganized, as well as the rubric not matching the format of the handout. 

## Final Thoughts
Despite these challenges, the lab successfully achieved its educational objectives. By providing practical experience with different testing methodologies, it reinforced the importance of systematic testing approaches in software development.
